{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRFbGGiZCzb1"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "f_7xdpyRLg9K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading librosa-0.10.2.post1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting soundfile\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\sonig\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.1.3)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Collecting pyaudio\n",
      "  Downloading PyAudio-0.2.14-cp313-cp313-win_amd64.whl.metadata (2.7 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting scipy>=1.2.0 (from librosa)\n",
      "  Downloading scipy-1.14.1-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=0.14 (from librosa)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\sonig\\appdata\\roaming\\python\\python313\\site-packages (from librosa) (5.1.1)\n",
      "Collecting numba>=0.51.0 (from librosa)\n",
      "  Downloading numba-0.60.0.tar.gz (2.7 MB)\n",
      "     ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 2.7/2.7 MB 20.5 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [24 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"C:\\Users\\sonig\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "          ~~~~^^\n",
      "        File \"C:\\Users\\sonig\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                                   ~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\sonig\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 118, in get_requires_for_build_wheel\n",
      "          return hook(config_settings)\n",
      "        File \"C:\\Users\\sonig\\AppData\\Local\\Temp\\pip-build-env-9uocun5u\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 334, in get_requires_for_build_wheel\n",
      "          return self._get_build_requires(config_settings, requirements=[])\n",
      "                 ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\sonig\\AppData\\Local\\Temp\\pip-build-env-9uocun5u\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 304, in _get_build_requires\n",
      "          self.run_setup()\n",
      "          ~~~~~~~~~~~~~~^^\n",
      "        File \"C:\\Users\\sonig\\AppData\\Local\\Temp\\pip-build-env-9uocun5u\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 522, in run_setup\n",
      "          super().run_setup(setup_script=setup_script)\n",
      "          ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\sonig\\AppData\\Local\\Temp\\pip-build-env-9uocun5u\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 320, in run_setup\n",
      "          exec(code, locals())\n",
      "          ~~~~^^^^^^^^^^^^^^^^\n",
      "        File \"<string>\", line 51, in <module>\n",
      "        File \"<string>\", line 48, in _guard_py_ver\n",
      "      RuntimeError: Cannot install on Python version 3.13.1; only versions >=3.9,<3.13 are supported.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa soundfile numpy scikit-learn pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TejNRFPhPvrs"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'librosa'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msoundfile\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mglob\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'librosa'"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import soundfile\n",
    "import os, glob, pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QI8oqg5JQeZZ"
   },
   "outputs": [],
   "source": [
    "# DataFlair - Extract features (mfcc, chroma, mel) from a sound file\n",
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate = sound_file.samplerate\n",
    "        if chroma:\n",
    "            stft = np.abs(librosa.stft(X))\n",
    "        result = np.array([])\n",
    "        if mfcc:\n",
    "            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result = np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "            result = np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T, axis=0)\n",
    "            result = np.hstack((result, mel))\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B6CLhgqOaXiC"
   },
   "outputs": [],
   "source": [
    "#DataFlair - Emotions in the RAVDESS dataset\n",
    "emotions={\n",
    "  '01':'neutral',\n",
    "  '02':'calm',\n",
    "  '03':'happy',\n",
    "  '04':'sad',\n",
    "  '05':'angry',\n",
    "  '06':'fearful',\n",
    "  '07':'disgust',\n",
    "  '08':'surprised'\n",
    "}\n",
    "\n",
    "#DataFlair - Emotions to observe\n",
    "observed_emotions=['calm', 'happy', 'fearful', 'disgust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oh33y8OaaaNo"
   },
   "outputs": [],
   "source": [
    "#DataFlair - Load the data and extract features for each sound file\n",
    "def load_data(test_size=0.2):\n",
    "    x,y=[],[]\n",
    "    for file in glob.glob(\"/content/drive/MyDrive/speech-emotion-recognition-ravdess-data/Actor_*/*.wav\"):\n",
    "        file_name=os.path.basename(file)\n",
    "        emotion=emotions[file_name.split(\"-\")[2]]\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        feature=extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        x.append(feature)\n",
    "        y.append(emotion)\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0We73UxGbDcr"
   },
   "outputs": [],
   "source": [
    "#DataFlair - Split the dataset\n",
    "x_train,x_test,y_train,y_test=load_data(test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MY2ny4RHeesp"
   },
   "outputs": [],
   "source": [
    "#DataFlair - Get the shape of the training and testing datasets\n",
    "print((x_train.shape[0], x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djf8cSRsemEw"
   },
   "outputs": [],
   "source": [
    "#DataFlair - Get the number of features extracted\n",
    "print(f'Features extracted: {x_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Qevb2lXeuBR"
   },
   "outputs": [],
   "source": [
    "#DataFlair - Initialize the Multi Layer Perceptron Classifier\n",
    "model=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlZgrA_XevzF"
   },
   "outputs": [],
   "source": [
    "#DataFlair - Train the model\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c21t5vlVe90u"
   },
   "outputs": [],
   "source": [
    "#DataFlair - Predict for the test set\n",
    "y_pred=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rGAY_gne_IS"
   },
   "outputs": [],
   "source": [
    "#DataFlair - Calculate the accuracy of our model\n",
    "accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "#DataFlair - Print the accuracy\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BjKQ9X1egchv"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a parameter grid to search over\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(100,), (200,), (300,)],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "\n",
    "    MLPClassifier(learning_rate='adaptive', max_iter=500),\n",
    "    param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    ")\n",
    "\n",
    "# Fit the model with the best hyperparameters\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train a model with the best hyperparameters\n",
    "best_model = MLPClassifier(learning_rate='adaptive', max_iter=500, **best_params)\n",
    "best_model.fit(x_train, y_train)\n",
    "y_pred = best_model.predict(x_test)\n",
    "accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "print(\"Best Model Accuracy: {:.2f}%\".format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5P50xpGTjUO3"
   },
   "outputs": [],
   "source": [
    "!pip install audiomentations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MU7V4RgDhevX"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Hyperparameter tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(100,), (200,), (300,), (400,)],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    MLPClassifier(max_iter=1000),  # Adjust max_iter as needed\n",
    "    param_grid,\n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "best_model = MLPClassifier(max_iter=1000, **best_params)\n",
    "best_model.fit(x_train, y_train)\n",
    "y_pred = best_model.predict(x_test)\n",
    "accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "print(\"Best Model Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VREmOHeDlKiv"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import librosa\n",
    "\n",
    "# Hyperparameter tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(100,), (200,), (300,), (400,)],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    MLPClassifier(max_iter=1000),  # Adjust max_iter as needed\n",
    "    param_grid,\n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "best_model = MLPClassifier(max_iter=1000, **best_params)\n",
    "best_model.fit(x_train, y_train)\n",
    "y_pred = best_model.predict(x_test)\n",
    "accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "print(\"Best Model Accuracy: {:.2f}%\".format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHiGVDVzxgU1"
   },
   "outputs": [],
   "source": [
    "import joblib,pickle\n",
    "\n",
    "# After training your model, save it to a file\n",
    "joblib.dump(model, '/content/drive/MyDrive/model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfcmpLNJxg4c"
   },
   "outputs": [],
   "source": [
    "model = joblib.load('/content/drive/MyDrive/model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtBEjEy-O3qz"
   },
   "outputs": [],
   "source": [
    "!pip install audiomentations ipywidgets matplotlib\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Audio\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import io\n",
    "import joblib\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "# Data augmentation using audiomentations\n",
    "augment = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.2, p=0.5),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "])\n",
    "\n",
    "# Extract features function (with mel, mfcc, and chroma)\n",
    "def extract_features(X, sample_rate, mfcc, chroma, mel):\n",
    "    if chroma:\n",
    "        stft = np.abs(librosa.stft(X))\n",
    "    result = np.array([])\n",
    "    if mfcc:\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result = np.hstack((result, mfccs))\n",
    "    if chroma:\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, chroma))\n",
    "    if mel:\n",
    "        mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, mel))\n",
    "    return result\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = joblib.load('/content/drive/MyDrive/model.pkl')\n",
    "\n",
    "# Define the user interface widgets\n",
    "file_upload = widgets.FileUpload(\n",
    "    accept='.wav',\n",
    "    multiple=False,\n",
    "    description='Upload WAV file'\n",
    ")\n",
    "\n",
    "recognize_button = widgets.Button(\n",
    "    description='Recognize Emotion',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "output_plot = widgets.Output()\n",
    "\n",
    "# Define the button click event handler\n",
    "def recognize_emotion(_):\n",
    "    file_contents = list(file_upload.value.values())[0]['content']\n",
    "    with sf.SoundFile(io.BytesIO(file_contents)) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate = sound_file.samplerate\n",
    "\n",
    "    feature = extract_features(X, sample_rate, mfcc=True, chroma=True, mel=True)\n",
    "    # Apply data augmentation to the feature\n",
    "    augmented_feature = augment(samples=feature, sample_rate=sample_rate)\n",
    "\n",
    "    # Predict emotion\n",
    "    emotions = model.predict_proba([augmented_feature])[0]\n",
    "\n",
    "    # Display the recognized emotions\n",
    "    with output_plot:\n",
    "        output_plot.clear_output(wait=True)\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        librosa.display.waveshow(X, sr=sample_rate)\n",
    "        plt.title(f'Waveform - Predicted Emotions')\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.show()\n",
    "\n",
    "        # Display emotion probabilities\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.bar(model.classes_, emotions)\n",
    "        plt.title('Emotion Probabilities')\n",
    "        plt.xlabel('Emotion')\n",
    "        plt.ylabel('Probability')\n",
    "        plt.show()\n",
    "\n",
    "# Attach the click event handler to the button\n",
    "recognize_button.on_click(recognize_emotion)\n",
    "\n",
    "# Display the user interface\n",
    "display(file_upload, recognize_button, output_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9VjB0ZZWQDj1"
   },
   "outputs": [],
   "source": [
    "'''!pip install audiomentations ipywidgets matplotlib\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Audio\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import io\n",
    "import joblib\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "# Data augmentation using audiomentations\n",
    "augment = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.2, p=0.5),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "])\n",
    "\n",
    "# Extract features function (with mel, mfcc, and chroma)\n",
    "def extract_features(X, sample_rate, mfcc, chroma, mel):\n",
    "    if chroma:\n",
    "        stft = np.abs(librosa.stft(X))\n",
    "    result = np.array([])\n",
    "    if mfcc:\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result = np.hstack((result, mfccs))\n",
    "    if chroma:\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, chroma))\n",
    "    if mel:\n",
    "        mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, mel))\n",
    "    return result\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = joblib.load('/content/drive/MyDrive/model.pkl')\n",
    "\n",
    "# Define the user interface widgets\n",
    "file_upload = widgets.FileUpload(\n",
    "    accept='.wav',\n",
    "    multiple=False,\n",
    "    description='Upload WAV file'\n",
    ")\n",
    "\n",
    "recognize_button = widgets.Button(\n",
    "    description='Recognize Emotion',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "output_plot = widgets.Output()\n",
    "\n",
    "# Define the button click event handler\n",
    "def recognize_emotion(_):\n",
    "    file_contents = list(file_upload.value.values())[0]['content']\n",
    "    with sf.SoundFile(io.BytesIO(file_contents)) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate = sound_file.samplerate\n",
    "\n",
    "    feature = extract_features(X, sample_rate, mfcc=True, chroma=True, mel=True)\n",
    "    # Apply data augmentation to the feature\n",
    "    augmented_feature = augment(samples=feature, sample_rate=sample_rate)\n",
    "\n",
    "    # Predict emotion\n",
    "    emotions = model.predict_proba([augmented_feature])[0]\n",
    "    classes = model.classes_\n",
    "\n",
    "    # Display the recognized emotions\n",
    "    with output_plot:\n",
    "        output_plot.clear_output(wait=True)\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        librosa.display.waveshow(X, sr=sample_rate)\n",
    "        plt.title(f'Waveform - Predicted Emotions')\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.show()\n",
    "\n",
    "        # Display emotion probabilities\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.bar(classes, emotions)\n",
    "        plt.title('Emotion Probabilities')\n",
    "        plt.xlabel('Emotion')\n",
    "        plt.ylabel('Probability')\n",
    "        plt.show()\n",
    "\n",
    "# Attach the click event handler to the button\n",
    "recognize_button.on_click(recognize_emotion)\n",
    "\n",
    "# Display the user interface\n",
    "display(file_upload, recognize_button, output_plot)'''\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
